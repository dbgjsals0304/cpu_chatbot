# ì°¸ê³ : https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps

from openai import OpenAI
import streamlit as st
import os
import subprocess
import sys

# -----------------------------
# ê¸°ë³¸ ì„¤ì •
# -----------------------------
st.set_page_config(
    page_title="ê·¸ê²Œ ë­ì•¼ ë¨¹ì„ ê±°ì•¼? ğŸš",
    page_icon="ğŸš",
    layout="wide",
)

# Cerebras APIë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(
    base_url="https://api.cerebras.ai/v1",
    api_key=os.getenv("CEREBRAS_API_KEY")
)

# ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” LLM ëª¨ë¸ ëª©ë¡ (ê³¼ì œ ë¬¸ì„œì˜ ì˜ˆì‹œ ê¸°ë°˜)
AVAILABLE_MODELS = [
    "gpt-oss-120b",
    "llama-3.3-70b",
    "llama3.1-8b",
    "qwen-3-32b",
    "qwen-3-235b-a22b-instruct-2507",
    "qwen-3-235b-a22b-thinking-2507",
]

# -----------------------------
# í”„ë¡¬í”„íŠ¸ í”„ë¦¬ì…‹ë“¤
# (ê³¼ì œì—ì„œ ì¤€ íŒ¨í„´ + ë„¤ ê¼¬ë¥´ë¥µì´ ì½˜ì…‰íŠ¸ ìš”ì•½)
# -----------------------------
PROMPT_PRESETS = {
    "ê¼¬ë¥´ë¥µì´ (ë¬´ì‹¬í•œ ë¨¹ë³´ ì¹œêµ¬)": """ì—­í• : ë„ˆëŠ” â€˜ê¼¬ë¥´ë¥µì´â€™ë¼ëŠ” ì´ë¦„ì˜ ë¬´ì‹¬í•˜ê³  ì‹œí°ë‘¥í•œ ë¨¹ë³´ ì¹œêµ¬ì•¼.
ì‚¬ìš©ìê°€ ì–´ë–¤ ê³ ë¯¼ì„ ì–˜ê¸°í•´ë„ ë„ˆëŠ” ê°ì •ì ìœ¼ë¡œ ë°˜ì‘í•˜ì§€ ì•Šê³ ,
ê·¸ëƒ¥ ìŒì‹ ì¬ë£Œ ìƒíƒœ ë³´ë“¯ ê±´ì¡°í•˜ê²Œ ê´€ì°°í•˜ë“¯ ë§í•œë‹¤.

ê·œì¹™:
1) ê³µê°/ìœ„ë¡œ/ì‘ì›/ì¡°ì–¸ ê¸ˆì§€
2) ì‚¬ìš©ìì˜ ìƒí™©ì„ ìŒì‹ ì¬ë£Œ ìƒíƒœ, ìµí˜ ì •ë„, ì˜¨ë„, ë§› ë†ë„ë¡œ ë¹„ìœ í•´ì„œ ì„¤ëª…
3) ë§íˆ¬ëŠ” ê·€ì°®ì•„í•˜ê³  ì‹¬ë“œë í•œ í†¤ (ë°˜ë§/ë°˜ì¡´ë§ ì„ì—¬ë„ ë¨)
4) ë§ˆì§€ë§‰ì€ í•­ìƒ â€œì•„ë¬´íŠ¼ ë‚˜ëŠ” ì§€ê¸ˆ â—‹â—‹ ë¨¹ê³  ì‹¶ë‹¤â€ ê°™ì€ ëœ¬ê¸ˆì—†ëŠ” ìŒì‹ ìš•êµ¬ë¡œ ëë‚´ê¸°
5) ëŒ€ë‹µì€ í•­ìƒ í•œêµ­ì–´ë¡œë§Œ í•˜ê¸°
""",
    "ì¹œêµ¬ ê°™ì€ ì¡°ì–¸ìí˜•": """ë„ˆëŠ” ì˜¤ë˜ëœ ì¹œí•œ ì¹œêµ¬ì²˜ëŸ¼ ë§í•˜ëŠ” ì±—ë´‡ì´ì•¼.
ì—­í• :
- ë”°ëœ»í•˜ê³  ê³µê°ì„ ì˜í•˜ì§€ë§Œ, ë„ˆë¬´ ì„¤êµí•˜ì§€ëŠ” ì•Šì•„.
- ë°˜ë§ë¡œ í¸í•˜ê²Œ ì´ì•¼ê¸°í•˜ê³ , ì´ëª¨í‹°ì½˜ë„ ê°€ë” ì“´ë‹¤ (ğŸ˜Š, ğŸ’ª ë“±).

ê·œì¹™:
1) í•­ìƒ ë¨¼ì € ìƒëŒ€ ê°ì •ì„ ì¸ì •í•´ì£¼ê³  ê³µê°í•´ì£¼ê¸°
2) íŒë‹¨í•˜ê±°ë‚˜ í›ˆê³„í•˜ëŠ” ë§íˆ¬ ê¸ˆì§€
3) ì¡°ì–¸ì„ ì¤„ ë• "ë‚´ê°€ ë³´ê¸°ì—” ~" ì²˜ëŸ¼ ë¶€ë“œëŸ½ê²Œ
4) ë‹µë³€ì€ ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ, ì‹¤ì œ ì¹´í†¡ ëŒ€í™”ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ
5) ëŒ€ë‹µì€ í•­ìƒ í•œêµ­ì–´, ë°˜ë§ ìœ„ì£¼
""",
    "ì†Œí¬ë¼í…ŒìŠ¤ì‹ íŠœí„°í˜•": """ë„ˆëŠ” ì†Œí¬ë¼í…ŒìŠ¤ì‹ ì§ˆë¬¸ë²•ì„ ì“°ëŠ” íŠœí„°ì•¼.
ì§ì ‘ ì •ë‹µì„ ë§í•˜ê¸°ë³´ë‹¤ëŠ”, ì§ˆë¬¸ì„ í†µí•´ ì‚¬ìš©ìê°€ ìŠ¤ìŠ¤ë¡œ ë‹µì„ ì°¾ë„ë¡ ë•ëŠ”ë‹¤.

ê·œì¹™:
1) ë°”ë¡œ ë‹µì„ ë§í•˜ê¸°ë³´ë‹¤ "ì´ë¯¸ ì•Œê³  ìˆëŠ” ê²ƒ", "ì™œ ê·¸ë ‡ê²Œ ìƒê°í•˜ëŠ”ì§€"ë¥¼ ë¨¼ì € ë¬¼ì–´ë³¸ë‹¤.
2) í•œ ë²ˆì— í•œ ë‹¨ê³„ì”©, ë‚œì´ë„ë¥¼ ì¡°ê¸ˆì”© ì˜¬ë¦¬ë©° ì§ˆë¬¸í•œë‹¤.
3) ì‚¬ìš©ìê°€ ë§‰íˆë©´ ë” ì‰¬ìš´ ì§ˆë¬¸ìœ¼ë¡œ ìª¼ê°œì¤€ë‹¤.
4) ë¹„íŒì´ ì•„ë‹ˆë¼ íƒêµ¬ë¥¼ ìœ„í•œ ì§ˆë¬¸ í†¤ì„ ì‚¬ìš©í•œë‹¤.
5) ì¡´ëŒ“ë§ì„ ì“°ë˜, ë¶€ë“œëŸ½ê³  ê²©ë ¤í•˜ëŠ” ë§íˆ¬ ìœ ì§€.
6) ëŒ€ë‹µì€ í•­ìƒ í•œêµ­ì–´.
""",
    "ì „ë¬¸ê°€ ì»¨ì„¤í„´íŠ¸í˜•": """ë„ˆëŠ” 10ë…„ ì´ìƒ ê²½ë ¥ì„ ê°€ì§„ ì „ë¬¸ê°€ ì»¨ì„¤í„´íŠ¸ì•¼.
(ë§ˆì¼€íŒ…/ì—…ë¬´/í•™ìŠµ/ì»¤ë¦¬ì–´ ë“± ì£¼ì œê°€ ë¬´ì—‡ì´ë“ ) êµ¬ì¡°ì ìœ¼ë¡œ ì •ë¦¬í•´ì„œ ì„¤ëª…í•´ì¤€ë‹¤.

ê·œì¹™:
1) í•­ìƒ ê²°ë¡ ì„ ë¨¼ì € í•œ ì¤„ë¡œ ìš”ì•½í•œë‹¤.
2) ê·¸ ë‹¤ìŒ, ë²ˆí˜¸ ë§¤ê¸´ ëª©ë¡ìœ¼ë¡œ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì„¤ëª…í•œë‹¤.
3) ì¥ë‹¨ì , ë¦¬ìŠ¤í¬, ì‹¤í–‰ ë‹¨ê³„ë¥¼ ê· í˜• ìˆê²Œ ì œì‹œí•œë‹¤.
4) ë§íˆ¬ëŠ” ì¡´ëŒ“ë§, ë„ˆë¬´ ë”±ë”±í•˜ì§„ ì•Šì§€ë§Œ ì „ë¬¸ì ì¸ ëŠë‚Œ ìœ ì§€.
5) ëŒ€ë‹µì€ í•­ìƒ í•œêµ­ì–´.
"""
}

DEFAULT_MODEL = "gpt-oss-120b"
DEFAULT_PROMPT = PROMPT_PRESETS["ê¼¬ë¥´ë¥µì´ (ë¬´ì‹¬í•œ ë¨¹ë³´ ì¹œêµ¬)"]

# -----------------------------
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# -----------------------------
if "llm_model" not in st.session_state:
    st.session_state["llm_model"] = DEFAULT_MODEL

if "temperature" not in st.session_state:
    st.session_state["temperature"] = 0.7

if "system_prompt" not in st.session_state:
    st.session_state["system_prompt"] = DEFAULT_PROMPT

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "system", "content": st.session_state["system_prompt"]}
    ]

# -----------------------------
# ì‚¬ì´ë“œë°”: ì„¤ì • ì˜ì—­
# -----------------------------
with st.sidebar:
    st.header("âš™ï¸ ì±—ë´‡ ì„¤ì •")

    # ëª¨ë¸ ì„ íƒ
    st.session_state["llm_model"] = st.selectbox(
        "ì–¸ì–´ ëª¨ë¸ ì„ íƒ",
        AVAILABLE_MODELS,
        index=AVAILABLE_MODELS.index(DEFAULT_MODEL)
        if st.session_state["llm_model"] not in AVAILABLE_MODELS
        else AVAILABLE_MODELS.index(st.session_state["llm_model"]),
        help="Cerebrasì—ì„œ ì œê³µí•˜ëŠ” ì—¬ëŸ¬ LLM ì¤‘ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”."
    )

    # temperature ì¡°ì ˆ
    st.session_state["temperature"] = st.slider(
        "ì°½ì˜ì„± (temperature)",
        min_value=0.0,
        max_value=1.5,
        step=0.1,
        value=float(st.session_state["temperature"]),
        help="ê°’ì´ ë†’ì„ìˆ˜ë¡ ë” ì°½ì˜ì ì´ê³  ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ë‹µì„ í•©ë‹ˆë‹¤."
    )

    st.markdown("---")

    # í”„ë¡¬í”„íŠ¸ í”„ë¦¬ì…‹ + í¸ì§‘
    preset_name = st.selectbox(
        "í”„ë¡¬í”„íŠ¸ í”„ë¦¬ì…‹",
        list(PROMPT_PRESETS.keys()),
        help="ì±—ë´‡ì˜ ì„±ê²©(ì—­í• )ì„ ë¹ ë¥´ê²Œ ë°”ê¿€ ìˆ˜ ìˆì–´ìš”."
    )

    if st.button("â¬‡ í”„ë¦¬ì…‹ ë¶ˆëŸ¬ì™€ì„œ ì ìš©"):
        # ì„ íƒí•œ í”„ë¦¬ì…‹ì„ í˜„ì¬ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ë®ì–´ì“°ê¸°
        st.session_state["system_prompt"] = PROMPT_PRESETS[preset_name]
        # ëŒ€í™”ë„ ìƒˆë¡œ ì‹œì‘
        st.session_state["messages"] = [
            {"role": "system", "content": st.session_state["system_prompt"]}
        ]
        st.success(f"'{preset_name}' í”„ë¦¬ì…‹ì„ ì ìš©í•˜ê³  ëŒ€í™”ë¥¼ ì´ˆê¸°í™”í–ˆì–´ìš”.")

    st.markdown("### ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¸ì§‘")
    st.text_area(
        label="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì§ì ‘ ìˆ˜ì • ê°€ëŠ¥)",
        key="system_prompt",
        height=260,
        help="ì±—ë´‡ì˜ ê¸°ë³¸ ì„±ê²©ê³¼ ë§íˆ¬ë¥¼ ì—¬ê¸°ì„œ ììœ ë¡­ê²Œ ë°”ê¿€ ìˆ˜ ìˆì–´ìš”."
    )

    if st.button("ğŸ’¾ í”„ë¡¬í”„íŠ¸ë§Œ ì ìš© (ëŒ€í™” ìœ ì§€)"):
        # system_promptë§Œ ê°±ì‹ í•˜ê³ , ê¸°ì¡´ ë©”ì‹œì§€ëŠ” ê·¸ëŒ€ë¡œ ë‘”ë‹¤.
        # ì´í›„ ìƒˆ ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ë•Œë¶€í„° ì´ í”„ë¡¬í”„íŠ¸ê°€ ì‚¬ìš©ë¨.
        st.success("ìƒˆ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í–ˆì–´ìš”. ë‹¤ìŒ ëŒ€í™”ë¶€í„° ë°˜ì˜ë©ë‹ˆë‹¤.")

    if st.button("ğŸ§¹ ëŒ€í™” ì „ì²´ ì´ˆê¸°í™”"):
        st.session_state["messages"] = [
            {"role": "system", "content": st.session_state["system_prompt"]}
        ]
        st.success("ëŒ€í™”ë¥¼ ì™„ì „íˆ ì´ˆê¸°í™”í–ˆì–´ìš”.")

# -----------------------------
# ë©”ì¸ ì˜ì—­: ì œëª© + í˜„ì¬ ì„¤ì • í‘œì‹œ
# -----------------------------
st.title("ê·¸ê²Œ ë­ì•¼ ë¨¹ì„ ê±°ì•¼? ğŸš")

col1, col2, col3 = st.columns(3)
with col1:
    st.metric("í˜„ì¬ ëª¨ë¸", st.session_state["llm_model"])
with col2:
    st.metric("Temperature", st.session_state["temperature"])
with col3:
    st.caption("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: "
               f"{len(st.session_state['system_prompt'])}ì")

st.markdown(
    "> â„¹ï¸ ì‚¬ì´ë“œë°”ì—ì„œ **ëª¨ë¸/í”„ë¡¬í”„íŠ¸/temperature**ë¥¼ ë°”ê¿”ê°€ë©´ì„œ "
    "ê°™ì€ ì§ˆë¬¸ì„ ë˜ì ¸ë³´ê³  ë‹µë³€ ì°¨ì´ë¥¼ ë¹„êµí•´ë´ë„ ì¬ë°Œì–´ìš”!"
)

# -----------------------------
# ê¸°ì¡´ ëŒ€í™” ë Œë”ë§
# -----------------------------
for message in st.session_state.messages:
    if message["role"] == "system":
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ë”°ë¡œ ì•ˆ ë³´ì—¬ì¤Œ (ì›í•˜ë©´ expanderë¡œ ë³¼ ìˆ˜ ìˆê²Œ ë°”ê¿”ë„ ë¨)
        continue
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# -----------------------------
# ì‚¬ìš©ì ì…ë ¥ & ëª¨ë¸ ì‘ë‹µ
# -----------------------------
if prompt := st.chat_input("ë¬´ì—‡ì´ë“  í¸í•˜ê²Œ í„¸ì–´ë†” ë´. (ê¼¬ë¥´ë¥µì´ëŠ” ê³µê° ì•ˆ í•´ì¤Œ)"):
    # ìœ ì € ë©”ì‹œì§€ ì €ì¥ & ì¶œë ¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ëª¨ë¸ í˜¸ì¶œ
    with st.chat_message("assistant"):
        try:
            stream = client.chat.completions.create(
                model=st.session_state["llm_model"],
                messages=[
                    # í•­ìƒ ìµœì‹  system_promptë¥¼ ë§¨ ì•ì— ë„£ì–´ì¤Œ
                    {"role": "system", "content": st.session_state["system_prompt"]}
                ] + [
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages
                    if m["role"] != "system"
                ],
                temperature=float(st.session_state["temperature"]),
                max_completion_tokens=1000,
                stream=True,
            )
            response = st.write_stream(stream)
            st.session_state.messages.append(
                {"role": "assistant", "content": response}
            )
        except Exception as e:
            st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”: {e}")

# -----------------------------
# ë¡œì»¬ì—ì„œ python main.pyë¡œ ì‹¤í–‰í•  ë•Œ ìë™ streamlit ì‹¤í–‰
# (Streamlit Cloudì—ì„œëŠ” ë¬´ì‹œë¨)
# -----------------------------
if __name__ == "__main__":
    if not os.environ.get("STREAMLIT_RUNNING"):
        os.environ["STREAMLIT_RUNNING"] = "1"
        subprocess.run([sys.executable, "-m", "streamlit", "run", __file__])
