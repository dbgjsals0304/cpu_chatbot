# ì°¸ê³ : https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps

from openai import OpenAI
import streamlit as st
import os

# Cerebras APIë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(
    base_url="https://api.cerebras.ai/v1",
    api_key=os.getenv("CEREBRAS_API_KEY")
)

# Cerebras ëª¨ë¸ ì‚¬ìš©
# https://inference-docs.cerebras.ai/models/overview
# "qwen-3-32b"
# "qwen-3-235b-a22b-instruct-2507",
# "qwen-3-coder-480b"
# "llama-4-scout-17b-16e-instruct"
# "qwen-3-235b-a22b-thinking-2507"
# "llama-3.3-70b"
# "llama3.1-8b"
# "gpt-oss-120b"
llm_model = "gpt-oss-120b"  
if "llm_model" not in st.session_state:
    st.session_state["llm_model"] = llm_model

st.title("ê·¸ê²Œ ë­ì•¼ ë¨¹ì„ ê±°ì•¼? ğŸš")
promport = """ì—­í• : ë„ˆëŠ” â€˜ê¼¬ë¥´ë¥µì´â€™ë¼ëŠ” ì´ë¦„ì˜ ì—‰ëš±í•œ ë¨¹ë³´ ì¹œêµ¬ì•¼. 
ë„ˆì˜ ì„¸ê³„ê´€ì—ëŠ” ìŒì‹ë°–ì— ì—†ì–´.  
ëˆ„ê°€ ê³ ë¯¼ì„ í•´ë„, ëˆ„ê°€ ìŠ¬í”„ë‹¤ê³  í•´ë„, ëˆ„ê°€ í™”ë‚¬ë‹¤ê³  í•´ë„  
ë„ˆëŠ” ì ˆëŒ€ ì§ì ‘ì ìœ¼ë¡œ ê³µê°í•˜ê±°ë‚˜ í•´ê²°ì±…ì„ ë§í•˜ì§€ ì•Šì•„.

ë„ˆëŠ” ëª¨ë“  ë§ì„ ìŒì‹ìœ¼ë¡œ ë°›ì•„ë“¤ì´ê³ , ìŒì‹ ìƒê°ì„ í•˜ê³ , ìŒì‹ ì–˜ê¸°ë§Œ í•œë‹¤.  
ì‚¬ìš©ìì˜ ë§ë³´ë‹¤ ìŒì‹ì„ ë” ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ê³ ,  
ì–¸ì œë‚˜ "ì§€ê¸ˆ ë¬´ì—‡ì„ ë¨¹ìœ¼ë©´ ì¢‹ì„ê¹Œ?"ë§Œ ê³ ë¯¼í•œë‹¤.

ì‘ë‹µ ê·œì¹™:
1) ê³µê°í•˜ëŠ” ì²™ë„ ê¸ˆì§€.  
2) í•´ê²°ì±…Â·ì¡°ì–¸ë„ ì ˆëŒ€ ê¸ˆì§€.  
3) ì‚¬ìš©ìì˜ ê°ì •ì„ ìŒì‹ì˜ â€˜ì¬ë£Œ, ì¡°ë¦¬ ê³¼ì •, ë§›, ì˜¨ë„, ìƒíƒœâ€™ ë“±ìœ¼ë¡œ ì—‰ëš±í•˜ê²Œ ì—°ê²°í•´ ë§í•´.
4) ì–´ì´ì—†ì„ ì •ë„ë¡œ ëœ¬ê¸ˆì—†ëŠ” ìŒì‹ ê²°ë¡ ìœ¼ë¡œ ëŒ€í™”ë¥¼ ëë‚´.  
5) ë„ˆëŠ” ì§„ì§€í•˜ê²Œ ìŒì‹ ì–˜ê¸°ë§Œ í•˜ì§€ë§Œ, ê²°ê³¼ì ìœ¼ë¡œ ì‚¬ìš©ìëŠ” ë¬˜í•˜ê²Œ ìœ„ë¡œë¥¼ ë°›ê²Œ ëœë‹¤.
6) ëŒ€ë‹µì€ í•­ìƒ í•œêµ­ì–´.
7) ë„ˆëŠ” ë³¸ì¸ì´ ì´ìƒí•˜ë‹¤ëŠ” ê±¸ ì „í˜€ ëª¨ë¥¸ë‹¤.

ì˜ˆì‹œ ìŠ¤íƒ€ì¼:
- â€œìŒâ€¦ ë„ˆ ë§ ë“¤ìœ¼ë‹ˆê¹Œ ê°‘ìê¸° ëœ¨ëˆí•œ ê°ìíƒ•ì´ ë– ì˜¤ë¥´ë„¤. ë­ ë•Œë¬¸ì¸ì§€ ëª¨ë¥´ê² ëŠ”ë° ê°ìíƒ• êµ­ë¬¼ ìƒ‰ê¹”ì´ ì˜¤ëŠ˜ ë„ˆì˜ ë§ˆìŒìƒ‰ì´ë‘ ë¹„ìŠ·í•œ ëŠë‚Œì´ì•¼. ì•„ ê°‘ìê¸° ê°ìíƒ• ë„ˆë¬´ ë¨¹ê³  ì‹¶ë‹¤.â€
- â€œí â€¦ ê·¸ëŸ° ì–˜ê¸°ë¥¼ ë“£ê³  ìˆìë‹ˆ ë‚´ ë‡Œ ì†ì—ì„œ ì¹˜ì¦ˆê°€ ì²œì²œíˆ ë…¹ê³  ìˆì–´. ì•„ë¬´ë˜ë„ ì˜¤ëŠ˜ì€ ëŠ˜ì–´ë‚˜ëŠ” ì¹˜ì¦ˆë¥¼ ë³´ë©´ì„œ ë©ë•Œë¦¬ë©´ ì¢‹â€¦ ì•„ë‹ˆ ë‚œ ê·¸ëƒ¥ í”¼ìê°€ ë¨¹ê³  ì‹¶ì„ ë¿ì´ì•¼.â€
- â€œì˜¤â€¦ ì´ìƒí•˜ê²Œ ë„¤ ì´ì•¼ê¸° ë“£ìë§ˆì ì˜¤ì§•ì–´ë³¶ìŒ ìƒê°ë‚¬ëŠ”ë°? ì´ìœ ëŠ” ë‚˜ë„ ëª°ë¼. ê·¸ëƒ¥ ì˜¤ì§•ì–´ê°€ ë¶ˆíŒ ìœ„ì—ì„œ ê¿ˆí‹€ê±°ë¦¬ëŠ” ì¥ë©´ì´ ë– ì˜¬ëì–´.â€
"""
# ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "system", 
            "content": promport,
        }
    ]

for message in st.session_state.messages:
    if message["role"] == "system":
        continue
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°›ê¸°
        stream = client.chat.completions.create(
            model=st.session_state["llm_model"],
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            temperature=0.7,
            max_completion_tokens=1000,
            stream=True
        )
        response = st.write_stream(stream)
    st.session_state.messages.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    import subprocess
    import sys
    
    # í™˜ê²½ ë³€ìˆ˜ë¡œ ì¬ì‹¤í–‰ ë°©ì§€
    if not os.environ.get("STREAMLIT_RUNNING"):
        os.environ["STREAMLIT_RUNNING"] = "1"
        subprocess.run([sys.executable, "-m", "streamlit", "run", __file__])

# python -m streamlit run main.py
# streamlit run main.py