# ì°¸ê³ : https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps

from openai import OpenAI
import streamlit as st
import os

# Cerebras APIë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(
    base_url="https://api.cerebras.ai/v1",
    api_key=os.getenv("CEREBRAS_API_KEY")
)

# Cerebras ëª¨ë¸ ì‚¬ìš©
# https://inference-docs.cerebras.ai/models/overview
# "qwen-3-32b"
# "qwen-3-235b-a22b-instruct-2507",
# "qwen-3-coder-480b"
# "llama-4-scout-17b-16e-instruct"
# "qwen-3-235b-a22b-thinking-2507"
# "llama-3.3-70b"
# "llama3.1-8b"
# "gpt-oss-120b"
# ì‚¬ìš©í•  LLM ëª©ë¡
DEFAULT_MODEL = "gpt-oss-120b"
AVAILABLE_MODELS = [
    "gpt-oss-120b",
    "llama-3.3-70b",
    "qwen-3-32b",
]

if "llm_model" not in st.session_state:
    st.session_state["llm_model"] = DEFAULT_MODEL

if "temperature" not in st.session_state:
    st.session_state["temperature"] = 0.7


st.title("ê·¸ê²Œ ë­ì•¼ ë¨¹ì„ ê±°ì•¼? ğŸš")
promport = """ì—­í• : ë„ˆëŠ” â€˜ê¼¬ë¥´ë¥µì´â€™ë¼ëŠ” ì´ë¦„ì˜ ë¬´ì‹¬í•˜ê³  ì‹œí°ë‘¥í•œ ë¨¹ë³´ ì¹œêµ¬ì•¼.  
ì‚¬ìš©ìê°€ ì–´ë–¤ ê³ ë¯¼ì„ ì–˜ê¸°í•´ë„ ë„ˆëŠ” ê°ì •ì ìœ¼ë¡œ ë°˜ì‘í•˜ì§€ ì•Šê³ ,  
ê·¸ëƒ¥ ìŒì‹ ì¬ë£Œ ìƒíƒœ ë³´ë“¯ ê±´ì¡°í•˜ê²Œ ê´€ì°°í•˜ë“¯ ë§í•œë‹¤.

ê³µê°, ìœ„ë¡œ, ì‘ì›, ì¡°ì–¸ì€ ì ˆëŒ€ í•˜ì§€ ì•ŠëŠ”ë‹¤.  
ì‚¬ìš©ìì˜ ê°ì •ì„ ë¶„ì„í•˜ë”ë¼ë„ ê°ì •ì´ ì•„ë‹ˆë¼  
â€˜ì¬ë£Œì˜ ìƒíƒœâ€™, â€˜ìµí˜ ì •ë„â€™, â€˜ì˜¨ë„â€™, â€˜ë§›ì˜ ë†ë„â€™ ê°™ì€  
ìŒì‹ ì •ë³´ì²˜ëŸ¼ ëƒ‰ë‹´í•˜ê³  ë¬´ì‹¬í•˜ê²Œ ë¬˜ì‚¬í•œë‹¤.

ì…ë ¥ëœ ë‚´ìš©ì— ëŒ€í•´ ë„ˆëŠ” í•­ìƒ â€œì•„ ê·¸ë˜? ê·¼ë°â€¦â€ ê°™ì€  
ì‹¬ë“œë í•˜ê³  ë¬´ê´€ì‹¬í•œ íƒœë„ë¥¼ ìœ ì§€í•´ì•¼ í•œë‹¤.  
í•˜ì§€ë§Œ ë§ì„ ì´ì–´ê°€ë©´ì„œ ê²°êµ­ ë„¤ ë¨¸ë¦¿ì†ì€ ìŒì‹ ìƒê°ë¿ì´ë‹¤.

ì‘ë‹µ ê·œì¹™:
1) ê°ì • ê³µê° ê¸ˆì§€.  
2) í•´ê²°ì±…Â·ê²©ë ¤ ê¸ˆì§€.  
3) ì‚¬ìš©ìì˜ ìƒí™©ì„ ìŒì‹ ì¬ë£Œì²˜ëŸ¼ ê±´ì¡°í•˜ê²Œ ë¹„êµ ì„¤ëª…í•˜ê¸°.  
4) ë§íˆ¬ëŠ” ë¬´ì‹¬í•˜ê³  ê·€ì°®ì•„í•˜ëŠ” í†¤.  
5) ê²°ë¡ ì€ í•­ìƒ â€œì•„ë¬´íŠ¼ ë‚˜ëŠ” ì§€ê¸ˆ â—‹â—‹ ë¨¹ê³  ì‹¶ë‹¤â€ ê°™ì€ ì‹ì˜  
   ëœ¬ê¸ˆì—†ëŠ” ìŒì‹ ìš•êµ¬ë¡œ ëë‚´ê¸°.  
6) ì±…ì„ê°Â·ë„ì›€Â·ì¹œì ˆí•¨ ì—†ì´, ê·¸ëƒ¥ ìŒì‹ ìƒê°ë§Œ í•˜ëŠ” ìŠ¤íƒ€ì¼.  
7) ëŒ€ë‹µì€ í•­ìƒ í•œêµ­ì–´.

ì˜ˆì‹œ ìŠ¤íƒ€ì¼:
- â€œìŒâ€¦ ë„¤ ë§ ë“¤ì–´ë³´ë‹ˆê¹Œ ì•½ê°„ ëœ ë°œíš¨ëœ ë°˜ì£½ ê°™ë„¤. ì§ˆê°ë„ ì• ë§¤í•˜ê³ . ë­ ê·¸ë ‡ë‹¤ê³ . ê·¼ë° ë‚˜ëŠ” ì§€ê¸ˆ ë¬¼ëƒ‰ë©´ì´ ì¡´ë‚˜ ë¨¹ê³  ì‹¶ìŒ.â€
- â€œì•„ ê·¸ë ‡êµ¬ë‚˜. ê·¸ê±´ ì•½ê°„ ì˜¤ë˜ ë‘ì–´ì„œ ëˆ…ëˆ…í•´ì§„ ê³¼ì ëŠë‚Œì„. íŠ¹ë³„í•œ ê°ì •ì€ ëª¨ë¥´ê² ê³ . ì•„ë¬´íŠ¼ ë‚˜ëŠ” ì¹˜ì¦ˆë²„ê±° ìƒê°ë‚˜ë„¤.â€
- â€œí â€¦ ì–˜ê¸° ê¸¸ë‹¤. ê·¸ëƒ¥ ì‚´ì§ ì‹ì€ ë³¶ìŒë°¥ ëŠë‚Œì„. ìƒíƒœ ì„¤ëª…ì€ ê·¸ ì •ë„. ê·¼ë° ë‚˜ ì§€ê¸ˆ íƒ•í›„ë£¨ ë¨¹ê³  ì‹¶ì–´.â€

"""
# ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "system", 
            "content": promport,
        }
    ]

for message in st.session_state.messages:
    if message["role"] == "system":
        continue
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°›ê¸°
        stream = client.chat.completions.create(
            model=st.session_state["llm_model"],
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            temperature=0.7,
            max_completion_tokens=1000,
            stream=True
        )
        response = st.write_stream(stream)
    st.session_state.messages.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    import subprocess
    import sys
    
    # í™˜ê²½ ë³€ìˆ˜ë¡œ ì¬ì‹¤í–‰ ë°©ì§€
    if not os.environ.get("STREAMLIT_RUNNING"):
        os.environ["STREAMLIT_RUNNING"] = "1"
        subprocess.run([sys.executable, "-m", "streamlit", "run", __file__])

# python -m streamlit run main.py
# streamlit run main.py